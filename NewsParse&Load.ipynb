{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61780e8f-536b-4b3e-824e-7eaa3d41aa38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from urllib.request import Request, urlopen\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "\n",
    "\n",
    "url = 'https://news.google.com/rss/search?q=\"bitcoin%20cash\"%20when%3A1d&hl=en-US&gl=US&ceid=US%3Aen'  #this rss gathers daily news about bitcoin cash \n",
    "\n",
    "\n",
    "class ParseFeed():\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.feed_url = url\n",
    "        \n",
    "\n",
    "    def parse(self):\n",
    "        '''\n",
    "        Parse the URL, and print all the details of the news \n",
    "        '''\n",
    "            \n",
    "\n",
    "        feeds = feedparser.parse(self.feed_url).entries\n",
    "        self.pubdate_list = [] # create callable lists\n",
    "        self.link_list = []\n",
    "        self.body_list = []\n",
    "        \n",
    "        # gather links from rss for article extraction\n",
    "        for f in feeds:\n",
    "            test_list=[]\n",
    "            test_list.append(f.get(\"link\"))\n",
    "            d=f.get(\"published_parsed\")\n",
    "            self.pubdate_list.append(d)\n",
    "            \n",
    "            #create some basic cleaning code\n",
    "            def tag_visible(element):\n",
    "                if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                    return False\n",
    "                if isinstance(element, Comment):\n",
    "                    return False\n",
    "                return True            \n",
    "            \n",
    "            #parse links in rss feed\n",
    "            for link in test_list:\n",
    "                def text_from_html(body):\n",
    "                    soup = BeautifulSoup(body, \"html.parser\")\n",
    "                    texts = soup.findAll(text=True)\n",
    "                    visible_texts = filter(tag_visible, texts)  \n",
    "                    return u\" \".join(t.strip() for t in visible_texts)\n",
    "                \n",
    "                # some websites really don't like spiders and other bots, while we can --we won't try to bypass them.\n",
    "                try:\n",
    "                    req = Request(link, headers={'user-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) ..'})\n",
    "                    html = urlopen(req).read()\n",
    "                    body_part = text_from_html(html)\n",
    "                    \n",
    "                    self.link_list.append(link)\n",
    "                    self.body_list.append(body_part)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "#initiate parser\n",
    "feed = ParseFeed(url)\n",
    "feed.parse()\n",
    "\n",
    "\n",
    "#convert struct date structure to timestamp\n",
    "pub_date_end = []\n",
    "for pdate in feed.pubdate_list:\n",
    "    pdate = str(datetime.fromtimestamp(mktime(pdate)))\n",
    "    pub_date_end.append(pdate)\n",
    "\n",
    "data_load =pd.DataFrame(list(zip(feed.link_list,pub_date_end, feed.body_list)), \n",
    "               columns =['link', 'date','body']) \n",
    "\n",
    "\n",
    "sent_score_list = []\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for paragraph in data_load['body']:\n",
    "      \n",
    "    \n",
    "    sentence_list = tokenize.sent_tokenize(paragraph)\n",
    "    paragraphSentiments = 0.0\n",
    "    for sentence in sentence_list:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        #print(\"{:-<69} {}\".format(sentence, str(vs[\"compound\"])))\n",
    "        paragraphSentiments += vs[\"compound\"]\n",
    "\n",
    "    try:\n",
    "        sent_score = round(paragraphSentiments / len(sentence_list), 3)\n",
    "        sent_score_list.append(sent_score)\n",
    "    except:\n",
    "        sent_score = 0\n",
    "        sent_score_list.append(sent_score)\n",
    "\n",
    "\n",
    "\n",
    "data_load['sentiment score'] = sent_score_list\n",
    "\n",
    "\n",
    "import azure.cosmos.cosmos_client as cosmos_client\n",
    "import azure.cosmos.exceptions as exceptions\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "import json\n",
    "\n",
    "\n",
    "#setup connection (information like this should be stored in secure place like KeyVault)\n",
    "client = cosmos_client.CosmosClient(\"https://trade-parser.documents.azure.com:443/\",\"{API key}\")\n",
    "db = client.create_database_if_not_exists(id= \"BCHSentimentDatabase\")\n",
    "\n",
    "container = db.create_container_if_not_exists(id=\"RSSparsedBCHnews\",partition_key=PartitionKey(path ='/date',kind ='Hash'))\n",
    "\n",
    "\n",
    "# load data to nosql\n",
    "\n",
    "for i in range(0,data_load.shape[0]):\n",
    "    # create a dictionary for the selected row\n",
    "    data_dict = dict(data_load.iloc[i,:])\n",
    "    # convert the dictionary to a json object.\n",
    "    data_dict = json.dumps(data_dict, default =str)\n",
    "    updated_item = container.upsert_item(json.loads(data_dict))\n",
    "    updated_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "940c3471-257d-423c-902a-aeed5896657c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "NewsParse&Load",
   "notebookOrigID": 2874969350990897,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
