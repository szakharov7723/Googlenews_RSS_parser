
import pandas as pd
import feedparser
from bs4 import BeautifulSoup
#import urllib.request
from bs4.element import Comment
from urllib.request import Request, urlopen
from datetime import datetime
from time import mktime

 

url = "https://news.google.com/rss/search?q=bch%20when%3A180d&hl=en-US&gl=US&ceid=US%3Aen"  #this rss gathers news about bitcoin cash for the last 180 days.


class ParseFeed():

    def __init__(self, url):
        self.feed_url = url
        

    def parse(self):
        '''
        Parse the URL, and print all the details of the news 
        '''
            

        feeds = feedparser.parse(self.feed_url).entries
        self.pubdate_list = [] # create callable lists
        self.link_list = []
        self.body_list = []
        
        # gather links from rss for article extraction
        for f in feeds:
            test_list=[]
            test_list.append(f.get("link"))
            d=f.get("published_parsed")
            self.pubdate_list.append(d)
            
            #create some basic cleaning code
            def tag_visible(element):
                if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:
                    return False
                if isinstance(element, Comment):
                    return False
                return True            
            
            #parse links in rss feed
            for link in test_list:
                def text_from_html(body):
                    soup = BeautifulSoup(body, "html.parser")
                    texts = soup.findAll(text=True)
                    visible_texts = filter(tag_visible, texts)  
                    return u" ".join(t.strip() for t in visible_texts)
                
                # some websites really don't like spiders and other bots, while we can --we won't try to bypass them.
                try:
                    req = Request(link, headers={'user-agent': 'Mozilla/5.0 (Windows NT 6.3; rv:36.0) ..'})
                    html = urlopen(req).read()
                    body_part = text_from_html(html)
                    
                    self.link_list.append(link)
                    self.body_list.append(body_part)
                except:
                    continue

#initiate parser
feed = ParseFeed(url)
feed.parse()


#convert struct date structure to timestamp
pub_date_end = []
for pdate in feed.pubdate_list:
    pdate = str(datetime.fromtimestamp(mktime(pdate)))
    pub_date_end.append(pdate)

#test parser
print(feed.link_list)




